{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform as tf\n",
    "import math \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define location of Images to Train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locationImages = ['/Users/gattia/Data/mri/ski10Dataset/TrainingData-A/', '/Users/gattia/Data/mri/ski10Dataset/TrainingData-C/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def importImageExtractArray(imageName):\n",
    "    flipper = sitk.FlipImageFilter()\n",
    "    flipper.SetFlipAxes([True, False, False])\n",
    "    image = sitk.ReadImage(imageName)\n",
    "    flippedImage = flipper.Execute(image)\n",
    "    imageArray = sitk.GetArrayFromImage(image)\n",
    "    flippedImageArray = sitk.GetArrayFromImage(flippedImage)\n",
    "    return(imageArray, flippedImageArray)\n",
    "\n",
    "def padImage(image, desiredShape):\n",
    "    shapeOriginal = image.shape\n",
    "    differenceX = desiredShape[0] - shapeOriginal[0]\n",
    "    differenceY = desiredShape[1] - shapeOriginal[1]\n",
    "    differenceZ = desiredShape[2] - shapeOriginal[2]\n",
    "    halfDiffX = differenceX/2\n",
    "    halfDiffY = differenceY/2\n",
    "    halfDiffZ = differenceZ/2\n",
    "    if (differenceX % 2 == 0): paddedArray = numpy.pad(image, [[halfDiffX, halfDiffX], [0,0], [0,0]], 'constant', constant_values=(0))\n",
    "    else: paddArray = numpy.pad(image, [[int(math.ceil(halfDiffX)), int(math.floor(halfDiffX))], [0,0], [0,0]], 'constant', constant_values=(0))\n",
    "    \n",
    "    if (differenceY % 2 == 0): paddedArray = numpy.pad(paddedArray, [[0,0],[halfDiffY, halfDiffY], [0,0]], 'constant', constant_values=(0))\n",
    "    else: paddedArray = numpy.pad(paddedArray, [[0,0],[int(math.ceil(halfDiffY)), int(math.floor(halfDiffY))], [0,0]], 'constant', constant_values=(0))\n",
    "    \n",
    "    if (differenceZ % 2 == 0): paddedArray = numpy.pad(paddedArray, [[0,0], [0,0], [halfDiffZ, halfDiffZ]], 'constant', constant_values=(0))\n",
    "    else: paddedArray = numpy.pad(paddedArray, [[0,0], [0,0], [int(math.ceil(halfDiffZ)), int(math.floor(halfDiffZ))]], 'constant', constant_values=(0))\n",
    "    \n",
    "    return(paddedArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import images and labels from the two folders that contain them. Flip each image in the AP direction. This will double the sample. \n",
    "\n",
    "* Save all images including flipped in imagesDictionary. \n",
    "* Save all labels in labelsDictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imagesDictionary = {}\n",
    "os.chdir(locationImages[0])\n",
    "imageNames = glob.glob('image-*.mhd')\n",
    "for imageName in imageNames:\n",
    "    imagesDictionary[imageName[:9]], imagesDictionary[imageName[:9] + '-Flipped'] = importImageExtractArray(imageName)\n",
    "\n",
    "os.chdir(locationImages[1])\n",
    "imageNames = glob.glob('image-*.mhd')\n",
    "for imageName in imageNames: \n",
    "    imagesDictionary[imageName[:9]], imagesDictionary[imageName[:9] + '-Flipped'] = importImageExtractArray(imageName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelsDictionary = {}\n",
    "os.chdir(locationImages[0])\n",
    "labelNames = glob.glob('labels-*.mhd')\n",
    "for labelName in labelNames:\n",
    "    labelsDictionary[labelName[:10]], labelsDictionary[labelName[:10] + '-Flipped'] = importImageExtractArray(labelName)\n",
    "\n",
    "os.chdir(locationImages[1])\n",
    "labelNames = glob.glob('labels-*.mhd')\n",
    "for labelName in labelNames:\n",
    "    labelsDictionary[labelName[:10]], labelsDictionary[labelName[:10] + '-Flipped'] = importImageExtractArray(labelName)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for image in imagesDictionary:\n",
    "#     print('imageName: ' + image + ' ; imageDimensions: ' + str(imagesDictionary[image].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minSize = [1000,1000,1000]\n",
    "# maxSize = [0,0,0]\n",
    "# for image in imagesDictionary:\n",
    "#     shapeImage = imagesDictionary[image].shape\n",
    "#     #print(numpy.asarray(shapeImage[1], dtype='float')/numpy.asarray(shapeImage[2], dtype='float'))\n",
    "#     minSize[0] = numpy.min([minSize[0], shapeImage[0]])\n",
    "#     minSize[1] = numpy.min([minSize[1], shapeImage[1]])\n",
    "#     minSize[2] = numpy.min([minSize[2], shapeImage[2]]) \n",
    "    \n",
    "#     maxSize[0] = numpy.max([maxSize[0], shapeImage[0]])\n",
    "#     maxSize[1] = numpy.max([maxSize[1], shapeImage[1]])\n",
    "#     maxSize[2] = numpy.max([maxSize[2], shapeImage[2]]) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code was run on this dataset to get the minimum and maximum dimensions. The result was: \n",
    "\n",
    "- MinSize is: [92, 314, 247]\n",
    "- MaxSize is: [120, 437, 343]\n",
    "\n",
    "This was used in determining how big to pad the images to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad every slice of each image so that the resulting slices are of shape 450,350. This is slightly bigger than the biggest in plane resolution. I didnt want to register each image as I thought this might make the algorithm more robust to differences in alignment etc. We'll see how it works out.\n",
    "\n",
    "The padding is done for both image and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 448, 352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for image in imagesDictionary:\n",
    "    imagesDictionary[image] = padImage(imagesDictionary[image], [len(imagesDictionary[image][:,1,1]), img_rows, img_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for label in labelsDictionary:\n",
    "    labelsDictionary[label] = padImage(labelsDictionary[label], [len(labelsDictionary[label][:,1,1]), img_rows, img_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.imshow(imagesDictionary[image][40,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for image in imagesDictionary:\n",
    "    imagesDictionary[image] = tf.resize(imagesDictionary[image], (len(imagesDictionary[image][:,1,1]), img_rows/2, img_cols/2), order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for label in labelsDictionary:\n",
    "    labelsDictionary[label] = tf.resize(labelsDictionary[label], (len(labelsDictionary[label][:,1,1]), img_rows/2, img_cols/2), order=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imageInputs= numpy.zeros([1,img_rows/2, img_cols/2])\n",
    "for image in imagesDictionary:\n",
    "    imageInputs = numpy.append(imageInputs, imagesDictionary[image], axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelInputs = numpy.zeros([1,img_rows/2, img_cols/2])\n",
    "for label in labelsDictionary:\n",
    "    labelInputs = numpy.append(labelInputs, labelsDictionary[label], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelInputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Dense, Dropout, Activation, Flatten, Reshape\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "# from keras import backend as K\n",
    "# from keras.utils import np_utils\n",
    "\n",
    "# def dice_coef(y_true, y_pred):\n",
    "#     y_true_f = K.flatten(y_true)\n",
    "#     y_pred_f = K.flatten(y_pred)\n",
    "#     intersection = K.sum(y_true_f * y_pred_f)\n",
    "#     return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "# def dice_coef_loss(y_true, y_pred):\n",
    "#     return -dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "# batch_size = 50\n",
    "# nb_classes = 5\n",
    "# nb_epoch = 10\n",
    "# #num_test_img = 100? \n",
    "\n",
    "# #input image dimensions - Defined above as img_rows and img_cols. 448x352\n",
    "\n",
    "# #Deconvnet\n",
    "# model = Sequential()\n",
    "\n",
    "# #conv layer 1, 224x176x1 -> 224x176x8\n",
    "# model.add(Convolution2D(8,5,5, border_mode = 'same', input_shape=(1,img_rows/2, img_cols/2)))\n",
    "# model.add(Activation('relu'))\n",
    "# #pooling layer 224x176x8 -> 112x88x8\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# #convv layer 2 112x88x8 -> 112x88x16\n",
    "# model.add(Convolution2D(16,5,5, border_mode='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# #pooling layer 113x88x16 -> 56x44x16\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# #deconv layer 1, 56x44x16 -> 56x44x8\n",
    "# model.add(Convolution2D(8,5,5, border_mode='same'))\n",
    "# model.add(Activation('relu'))       \n",
    "# #model.add(Deconvolution2D(8,5,5, output_shape=(batch_size, 8,112,88), subsample=(2,2), border_mode='same'))\n",
    "\n",
    "# # unpooling 56x44x8 -> 112x88x8\n",
    "# model.add(UpSampling2D(size=(2,2)))\n",
    "# #model.add(Deconvolution2D(8,5,5,output_shape=(batch_size, 8,224,176), subsample=(2,2), border_mode='same'))\n",
    "\n",
    "# #deconvolv layer 2 112x88x8 -> 112x88x8\n",
    "# model.add(Convolution2D(8,5,5, border_mode='same'))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# #unpooling 112x88x8 -> 224x164x8\n",
    "# model.add(UpSampling2D(size=(2,2)))\n",
    "\n",
    "# #deconv layer 3 224x164x8 -> 224x164x5\n",
    "# model.add(Convolution2D(5,1,1, border_mode='same'))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# #reshape output for learning purposes\n",
    "# model.add(Reshape((5, (224)*(164))))\n",
    "# model.add(Permute((2,1)))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# #training phase\n",
    "# model.compile(loss=dice_coef_loss, optimizer='adam',  metrics=[dice_coef])\n",
    "\n",
    "# model.fit(imageInputs, labelInputs, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, UpSampling2D, Activation, Permute, Flatten, Reshape\n",
    "#from keras.optimizers import Adam\n",
    "#from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "# from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "nb_classes = 5\n",
    "nb_epoch = 10\n",
    "#num_test_img = 100? \n",
    "\n",
    "#input image dimensions - Defined above as img_rows and img_cols. 448x352\n",
    "\n",
    "#For Dice\n",
    "smooth = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Deconvnet\n",
    "model = Sequential()\n",
    "\n",
    "# 1. conv layer 1, 224x176x1 -> 224x176x8\n",
    "model.add(Convolution2D(8,5,5, border_mode = 'same', input_shape=(1,img_rows/2, img_cols/2)))\n",
    "model.add(Activation('relu'))\n",
    "# 2. pooling layer 224x176x8 -> 112x88x8\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# 3. conv layer 1, 112x88x8 -> 112x88x8\n",
    "model.add(Convolution2D(8,5,5, border_mode='same'))\n",
    "model.add(Activation('relu'))       \n",
    "#model.add(Deconvolution2D(8,5,5, output_shape=(batch_size, 8,112,88), subsample=(2,2), border_mode='same'))\n",
    "\n",
    "# 4. unpooling 112x88x8 -> 224x176x8\n",
    "model.add(UpSampling2D(size=(2,2)))\n",
    "#model.add(Deconvolution2D(8,5,5,output_shape=(batch_size, 8,224,176), subsample=(2,2), border_mode='same'))\n",
    "\n",
    "# 5. deConv layer 3 224x176x8 -> 224x176x5\n",
    "model.add(Convolution2D(5,1,1, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#reshape output for learning purposes\n",
    "model.add(Reshape((5, (img_rows/2)*(img_cols/2))))\n",
    "model.add(Permute((2,1)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#training phase\n",
    "model.compile(loss=dice_coef_loss, optimizer='adam',  metrics=[dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(imageInputs, labelInputs, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # validation\n",
    "# print(\"Validating...\")\n",
    "# val_loss, val_accuracy = model.evaluate(data_test, label_test, show_accuracy=True, verbose=1)\n",
    "# print('Validation Accuracy: ', str(val_accuracy))\n",
    "# # prediction\n",
    "# print(\"Predicting...\")\n",
    "# preds = model.predict_proba(data_test, verbose=1) # reshape will be done later"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
