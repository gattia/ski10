{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import SimpleITK as sitk\n",
    "import numpy\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform as tf\n",
    "import math \n",
    "from hurry.filesize import size\n",
    "from hurry.filesize import alternative\n",
    "from sys import getsizeof\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define location of Images to Train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locationImages = ['/Users/gattia/Data/mri/ski10Dataset/TrainingData-A/', '/Users/gattia/Data/mri/ski10Dataset/TrainingData-C/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def importImageExtractArray(imageName):\n",
    "    flipper = sitk.FlipImageFilter()\n",
    "    flipper.SetFlipAxes([True, False, False])\n",
    "    image = sitk.ReadImage(imageName)\n",
    "    flippedImage = flipper.Execute(image)\n",
    "    imageArray = sitk.GetArrayFromImage(image)\n",
    "    flippedImageArray = sitk.GetArrayFromImage(flippedImage)\n",
    "    return(imageArray, flippedImageArray)\n",
    "\n",
    "def padImage(image, desiredShape):\n",
    "    shapeOriginal = image.shape\n",
    "    differenceX = desiredShape[0] - shapeOriginal[0]\n",
    "    differenceY = desiredShape[1] - shapeOriginal[1]\n",
    "    differenceZ = desiredShape[2] - shapeOriginal[2]\n",
    "    halfDiffX = differenceX/2\n",
    "    halfDiffY = differenceY/2\n",
    "    halfDiffZ = differenceZ/2\n",
    "    #Pad x- dimension\n",
    "    if (differenceX % 2 == 0): \n",
    "        paddedArray = numpy.pad(image, [[int(halfDiffX), int(halfDiffX)], [0,0], [0,0]], 'constant', constant_values=(0))\n",
    "    else: \n",
    "        paddedArray = numpy.pad(image, [[int(math.ceil(halfDiffX)), int(math.floor(halfDiffX))], [0,0], [0,0]], 'constant', constant_values=(0))\n",
    "    \n",
    "    #pad y-dimension\n",
    "    if (differenceY % 2 == 0): \n",
    "        paddedArray = numpy.pad(paddedArray, [[0,0],[int(halfDiffY), int(halfDiffY)], [0,0]], 'constant', constant_values=(0))\n",
    "    else: \n",
    "        paddedArray = numpy.pad(paddedArray, [[0,0],[int(math.ceil(halfDiffY)), int(math.floor(halfDiffY))], [0,0]], 'constant', constant_values=(0))\n",
    "    \n",
    "    #pad z-dimension\n",
    "    if (differenceZ % 2 == 0): \n",
    "        paddedArray = numpy.pad(paddedArray, [[0,0], [0,0], [int(halfDiffZ), int(halfDiffZ)]], 'constant', constant_values=(0))\n",
    "    else: \n",
    "        paddedArray = numpy.pad(paddedArray, [[0,0], [0,0], [int(math.ceil(halfDiffZ)), int(math.floor(halfDiffZ))]], 'constant', constant_values=(0))\n",
    "    \n",
    "    return(paddedArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import images and labels from the two folders that contain them. Flip each image in the AP direction. This will double the sample. \n",
    "\n",
    "* Save all images including flipped in imagesDictionary. \n",
    "* Save all labels in labelsDictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def memoryDictionary(dictionary):\n",
    "    memory = 0\n",
    "    for item in dictionary:\n",
    "        memory += dictionary[item].nbytes\n",
    "    sensicleMemory = size(memory, system=alternative)\n",
    "    return(sensicleMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imagesDictionary = {}\n",
    "os.chdir(locationImages[0])\n",
    "imageNames = glob.glob('image-*.mhd')\n",
    "for imageName in imageNames:\n",
    "    imagesDictionary[imageName[:9]], imagesDictionary[imageName[:9] + '-Flipped'] = importImageExtractArray(imageName)\n",
    "\n",
    "os.chdir(locationImages[1])\n",
    "imageNames = glob.glob('image-*.mhd')\n",
    "for imageName in imageNames: \n",
    "    imagesDictionary[imageName[:9]], imagesDictionary[imageName[:9] + '-Flipped'] = importImageExtractArray(imageName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int16')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagesDictionary[imageName[:9]].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 GB\n"
     ]
    }
   ],
   "source": [
    "print(memoryDictionary(imagesDictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelsDictionary = {}\n",
    "os.chdir(locationImages[0])\n",
    "labelNames = glob.glob('labels-*.mhd')\n",
    "for labelName in labelNames:\n",
    "    labelsDictionary[labelName[:10]], labelsDictionary[labelName[:10] + '-Flipped'] = importImageExtractArray(labelName)\n",
    "\n",
    "os.chdir(locationImages[1])\n",
    "labelNames = glob.glob('labels-*.mhd')\n",
    "for labelName in labelNames:\n",
    "    labelsDictionary[labelName[:10]], labelsDictionary[labelName[:10] + '-Flipped'] = importImageExtractArray(labelName)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GB\n"
     ]
    }
   ],
   "source": [
    "print(memoryDictionary(labelsDictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsDictionary[labelName[:10]].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code was run on this dataset to get the minimum and maximum dimensions. The result was: \n",
    "\n",
    "- MinSize is: [92, 314, 247]\n",
    "- MaxSize is: [120, 437, 343]\n",
    "\n",
    "This was used in determining how big to pad the images to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad every slice of each image so that the resulting slices are of shape 450,350. This is slightly bigger than the biggest in plane resolution. I didnt want to register each image as I thought this might make the algorithm more robust to differences in alignment etc. We'll see how it works out.\n",
    "\n",
    "The padding is done for both image and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 448, 352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for image in imagesDictionary:\n",
    "    imagesDictionary[image] = padImage(imagesDictionary[image], [len(imagesDictionary[image][:,1,1]), img_rows, img_cols])\n",
    "    imagesDictionary[image] = tf.resize(imagesDictionary[image], (len(imagesDictionary[image][:,1,1]), img_rows/2, img_cols/2), order=1)\n",
    "    labelName = ('labels-' + image[6:])\n",
    "    labelsDictionary[labelName] = padImage(labelsDictionary[labelName], [len(labelsDictionary[labelName][:,1,1]), img_rows, img_cols])\n",
    "    labelsDictionary[labelName] = tf.resize(labelsDictionary[labelName], (len(labelsDictionary[labelName][:,1,1]), img_rows/2, img_cols/2), order=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 GB\n",
      "5 GB\n"
     ]
    }
   ],
   "source": [
    "print(memoryDictionary(imagesDictionary))\n",
    "print(memoryDictionary(labelsDictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17428.]\n"
     ]
    }
   ],
   "source": [
    "noSlices = numpy.zeros([1])\n",
    "for image in imagesDictionary:\n",
    "    noSlices = noSlices + len(imagesDictionary[image][:,1,1])\n",
    "print(noSlices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gattia/anaconda2/envs/tf/lib/python2.7/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  if __name__ == '__main__':\n",
      "/Users/gattia/anaconda2/envs/tf/lib/python2.7/site-packages/ipykernel/__main__.py:2: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving .npy files done\n"
     ]
    }
   ],
   "source": [
    "train_images = numpy.ndarray((noSlices, img_rows/2, img_cols/2), dtype=imagesDictionary[image].dtype)\n",
    "train_labels = numpy.ndarray((noSlices, img_rows/2, img_cols/2), dtype=labelsDictionary[labelName].dtype)\n",
    "index=0\n",
    "for image in imagesDictionary:\n",
    "    imageSlices = len(imagesDictionary[image][:,1,1])\n",
    "#     for imageSlice in range(len(imagesDictionary[image][:,1,1])):\n",
    "    train_images[index:index+imageSlices] = imagesDictionary[image]\n",
    "    train_labels[index:index+imageSlices] = labelsDictionary[('labels-' + image[6:])]\n",
    "    index +=imageSlices\n",
    "    imagesDictionary[image] = None\n",
    "    labelsDictionary[('labels-' + image[6:])] = None\n",
    "\n",
    "numpy.save('/Users/gattia/Data/mri/ski10Dataset/train_images.npy', train_images)\n",
    "numpy.save('/Users/gattia/Data/mri/ski10Dataset/train_labels.npy', train_labels)\n",
    "print('Saving .npy files done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17428, 224, 176)\n",
      "(17428, 224, 176)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "imagesDictionary = None\n",
    "labelsDictionary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17428, 1, 224, 176)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images.shape[0],1,train_images.shape[1], train_images.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17428, 1, 224, 176)\n",
      "(17428, 1, 224, 176)\n"
     ]
    }
   ],
   "source": [
    "train_images = numpy.expand_dims(train_images, axis=1)\n",
    "train_labels = numpy.expand_dims(train_labels, axis=1)\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving .npy files done\n"
     ]
    }
   ],
   "source": [
    "numpy.save('/Users/gattia/Data/mri/ski10Dataset/train_images.npy', train_images)\n",
    "numpy.save('/Users/gattia/Data/mri/ski10Dataset/train_labels.npy', train_labels)\n",
    "print('Saving .npy files done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, UpSampling2D, Activation, Permute, Flatten, Reshape\n",
    "#from keras.optimizers import Adam\n",
    "#from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "# from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "nb_classes = 5\n",
    "nb_epoch = 10\n",
    "#num_test_img = 100? \n",
    "\n",
    "#input image dimensions - Defined above as img_rows and img_cols. 448x352\n",
    "\n",
    "#For Dice\n",
    "smooth = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Deconvnet\n",
    "model = Sequential()\n",
    "\n",
    "# 1. conv layer 1, 224x176x1 -> 224x176x8\n",
    "model.add(Convolution2D(8,5,5, border_mode = 'same', input_shape=(1,(img_rows/2), (img_cols/2))))\n",
    "model.add(Activation('relu'))\n",
    "# 2. pooling layer 224x176x8 -> 112x88x8\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# 3. conv layer 1, 112x88x8 -> 112x88x8\n",
    "model.add(Convolution2D(8,5,5, border_mode='same'))\n",
    "model.add(Activation('relu'))       \n",
    "#model.add(Deconvolution2D(8,5,5, output_shape=(batch_size, 8,112,88), subsample=(2,2), border_mode='same'))\n",
    "\n",
    "# 4. unpooling 112x88x8 -> 224x176x8\n",
    "model.add(UpSampling2D(size=(2,2)))\n",
    "#model.add(Deconvolution2D(8,5,5,output_shape=(batch_size, 8,224,176), subsample=(2,2), border_mode='same'))\n",
    "\n",
    "# 5. deConv layer 3 224x176x8 -> 224x176x5\n",
    "model.add(Convolution2D(5,1,1, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#reshape output for learning purposes\n",
    "model.add(Reshape((5, (img_rows/2)*(img_cols/2))))\n",
    "model.add(Permute((2,1)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#training phase\n",
    "model.compile(loss=dice_coef_loss, optimizer='adam',  metrics=[dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(imageInputs, labelInputs, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # validation\n",
    "# print(\"Validating...\")\n",
    "# val_loss, val_accuracy = model.evaluate(data_test, label_test, show_accuracy=True, verbose=1)\n",
    "# print('Validation Accuracy: ', str(val_accuracy))\n",
    "# # prediction\n",
    "# print(\"Predicting...\")\n",
    "# preds = model.predict_proba(data_test, verbose=1) # reshape will be done later"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
